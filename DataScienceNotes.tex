%        File: 
%     Created: Thu Sep 17 10:00 PM 2015 C
% Last Change: Thu Sep 17 10:00 PM 2015 C
%
\documentclass[11pt,letterpaper,oneside]{memoir}
\usepackage[margin=2cm]{geometry}
\usepackage{verbatim}
\usepackage{thmtools}
\usepackage{amsthm}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
% create environments for definitions and questions
\theoremstyle{definition}
\newtheorem{defn}{Definition}

\usepackage[nogroupskip,acronym]{glossaries}
\makenoidxglossaries
\loadglsentries{DataScienceNotesGA}
\printnoidxglossaries
%\printacronyms
\printglossary[type=\acronymtype]

\usepackage[T1]{fontenc}
\usepackage{newtxtext,newtxmath}
\usepackage{url}
\usepackage[svgnames]{xcolor}
\ifpdf
  \usepackage{pdfcolmk}
\fi
%% check if using xelatex rather than pdflatex
\ifxetex
  \usepackage{fontspec}
\fi
\usepackage{graphicx}
\usepackage[]{biblatex}
\usepackage{hyperref}
%% drawing package
\usepackage{tikz}
%% for dingbats
\usepackage{pifont}
\usepackage{csquotes}
\providecommand{\HUGE}{\Huge}% if not using memoir
\newlength{\drop}% for my convenience
\newcommand*{\TXfont}[1]{\fontencoding{T1}\fontfamily{#1}\selectfont}
%% Generic publisherâ€™s logo
\newcommand*{\plogo}{\fbox{$\mathcal{YW}$}}

\newcommand{\etal}{\textit{et al}.}
\newcommand{\ie}{\textit{i}.\textit{e}. }
\newcommand{\eg}{\textit{e}.\textit{g}. }

\input{title}
\makeindex
\addbibresource{DataScienceNotes.bib}
\begin{document}
\pagestyle{empty}
\titleBC
\clearpage
\tableofcontents
\chapter*{Prologue}
There is content that I'm trying to learn in the pursuit of data science. This
note book serves the following purposes:
\begin{enumerate}
  \item A log to reflect, analyze synthesize and in general integrate new
    material as I'm brought into contact with it.  Each chapter is a topic
    referenced in Zipfian's Data Science Primer \cite{data_science_primer}, or
    an abstraction - as an example 'Software Development' is used instead of
    Python programming because there are subcategories of equal importance to
    me than Python programming, for instance algorithms.
  \item Have a catalog of what I've studied up until entering into the boot
    camp.
  \item Be honest with the quantity and quality of work I've produced.
\end{enumerate}

\chapter{Computer Science}
\section{Algorithms}
\section{Machine Learning}

%\begin{description}
%  \item[Task]
%    An abstract representation of a problem we want to solve regarding the domain objects of import.
%  \item[decision rule]
%    given an observable random variable $x$ over the probability space  $(\mathcal{x},\sigma, p_\theta)$, determined by a parameter $\theta \in \theta$, and a set a of possible actions, a (deterministic) decision rule is a function $\delta : \mathcal{x} \rightarrow a$.
%  \item[Domain objects]
%    Elements of an intuitive model prior to applying machine learning to it, \eg emails in a spam filter, movies in a rating system
%  \item[Feature representation]
%    Describe the relevant objects in our domains for use in a machine learning model.
%  \mdef{Machine learning}{
%      \item The systematic study of algorithms and systems that improve their knowledge or performance with experience.
%      \item Using the right features to build the right models that achieve the right tasks
%      }
%  \item[Generalization]
%    being able to use training data as experience to improve on the outcome of a given task
%  \mdef{Model}{
%    \item A map from data points to outputs
%    \item is itself produced as the output of a machine learning algorithm applied to training data
%    }
%  \item[Posterior probability]
%    probability taken after a given feature was observed
%\end{description}

\begin{quote}
  models lend the machine learning field diversity, but tasks and features give
  it unity. \cite[p. 13]{Flach2012}
\end{quote}

\begin{quote}
  Machine learning is all about using the proper algorithms to accomplish the
  desired \glspl{task}.
\end{quote}

\begin{quote}
  A useful rule of thumb is: use likelihoods if you want to ignore the prior
  distribution or assume it uniform, and posterior probabilities otherwise.
  \cite[p. 28]{Flach2012} 
\end{quote}

\begin{comment}
\begin{defn}[Task]
  \label{defn:task}
  An abstract representation of a problem we want to solve regarding the domain objects of import.
\end{defn}
\end{comment}

\chapter{Software Engineering}
\section{Good Code}
LeBlanc's law: Later equals never.

Martin, Robert C. (2008-08-01). Clean Code: A Handbook of Agile Software Craftsmanship (Kindle Location 553). Pearson Education. Kindle Edition. 

What is good code?

From the introduction it appears there are some qualities that if enough are satisfied it should be called good code.  What is interesting is that good code can be distinguished quite easily.  Good code should be,

\begin{itemize}
  \item readable, and understandable
  \item elegantly solves the task it was built to complete 
  \item has a full suite of tests which it passes
\end{itemize}
The stress of the authors of \cite{martin2009clean} appear to be making the connection between good and clean code.  What good code is, is still open for debate what the authors provide are behaviors they've learned through their study.  That is read it to learn but don't expect it to contain absolute truth about coding well.
\section{Python}
There is a Pythonic way of coding.  This is a culturally obtained set of standards.  Values are,

\begin{itemize}
  \item explicit
  \item choose simple over complex
  \item maximize readability
\end{itemize}

\section{SQL}
\chapter{Mathematics}
\section{Probability}
\section{Statistics}
\section{Linear Algebra}
\chapter{Social Sciences}
As data science is not just the facts, and not only what we do with them, but
how we communicate them to others the social sciences play a large part in
succeeding in such a position.  Notable books 

\printnoidxglossaries
%\printacronyms
\bibliographystyle{plain}
\end{document}
Notes to self:
I have Imposter Syndrome.
\begin{comment}
  \item[Model]
    \mbox{}\\[-1.7\baselineskip]
    \begin{enumerate}
      \item A map from data points to outputs
      \item is itself produced as the output of a machine learning algorithm applied to training data

## Example: SpamAssassin: Spam vs Ham

+ Linear Classifier
* decision rule:
$\mathbf{w}\cdot\mathbf{x} > t$, where the message is deemed spam if the lhs is greater than the rhs
* 
decision boundary (plane): 
$\mathbf{w}\cdot\mathbf{x} = t$
separates spam from ham
* homogeneous equation by addition of constant variable $x_0=1$ where $w_0=-t$, so that the decision rule for example can be written, 
\begin{equation} 
  {\mathbf{w}^\circ} \cdot {\mathbf{x}^\circ} > 0 
\end{equation}
, where  $\mathbf{w}^\circ = (-t, w_1, w_2, ..., w_n)$ and $\mathbf{x}^\circ = (1, x_1, x_2, ..., x_n)$.
+ Overfitting
* Over training for too specialized a case


# Chapter 1

## Quotes


## 1.1 Tasks: the problems that can be solved with machine learning


## Models: 
\begin{thm}
  just a test
\end{thm}

\end{comment}
